{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b6928d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "# 导入必要的库\n",
    "from pandas import read_csv, concat\n",
    "from dateutil.parser import parse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Bidirectional, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit, ParameterGrid\n",
    "from scipy.interpolate import CubicSpline\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import MeanAbsolutePercentageError\n",
    "import seaborn as sns\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from scipy.optimize import minimize\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as scs\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error, mean_squared_error, mean_squared_log_error\n",
    "\n",
    "# 配置 Matplotlib 正常显示中文标签和负号\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号\n",
    "\n",
    "# 读取新能源汽车销量数据\n",
    "df_sale = pd.read_excel('E:\\\\qjy\\\\ecnu\\\\毕业论文\\\\新能源汽车数据\\\\中国-新能源汽车数据-已处理\\\\中国新能源汽车销量及其影响因素-已合并.xlsx', index_col=0)\n",
    "df_sale.head()\n",
    "\n",
    "# 计算相关系数并筛选相关性较高的变量\n",
    "correlation_matrix = df_sale.corr()\n",
    "relevant_vars = correlation_matrix.loc[:, 'nev_sale'][abs(correlation_matrix['nev_sale']) > 0.5].index\n",
    "df_sale1 = df_sale[relevant_vars]\n",
    "\n",
    "# 样条插值函数\n",
    "def spline_interpolation(series):\n",
    "    known_dates = series.dropna().index.to_julian_date()\n",
    "    known_values = series.dropna().values\n",
    "    cubic_spline = CubicSpline(known_dates, known_values)\n",
    "    interpolated_values = cubic_spline(series.index.to_julian_date())\n",
    "    return interpolated_values\n",
    "\n",
    "# 对每列进行插值处理\n",
    "for column in df_sale1.columns:\n",
    "    df_sale1[column] = spline_interpolation(df_sale1[column])\n",
    "\n",
    "# 去除充电桩数量缺失的行，从2016年1月开始\n",
    "df_sale2 = df_sale1[24:]\n",
    "\n",
    "# 绘制变量的折线图\n",
    "for column in df_sale2.columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(df_sale2.index, df_sale2[column], label=column)\n",
    "    plt.title('{}图'.format(column))\n",
    "    plt.xlabel('时间')\n",
    "    plt.ylabel('值')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# 创建X和Y数据集的函数\n",
    "def createXY(dataset, look_back=12):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(dataset) - look_back - 11):\n",
    "        a = dataset[i:(i+look_back), :]\n",
    "        b = dataset[(i+look_back):(i+look_back+12), 0]\n",
    "        X.append(a)\n",
    "        Y.append(b)\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "# 参数网格\n",
    "param_grid = {\n",
    "    'neurons': [5, 10, 15, 20],\n",
    "    'batch_size': [8, 16, 32],\n",
    "    'epochs': [100, 200, 500],\n",
    "    'learn_rate': [0.001, 0.01],\n",
    "    'drop_out': [0.1, 0.2]\n",
    "}\n",
    "\n",
    "# 创建参数网格\n",
    "grid = ParameterGrid(param_grid)\n",
    "\n",
    "# 存储每次迭代的结果\n",
    "results = []\n",
    "\n",
    "# 逐一遍历参数网格并训练模型\n",
    "for params in grid:\n",
    "    for repetition in range(10):\n",
    "        print(f'Current params: {params}, repetition {repetition + 1}/10')\n",
    "\n",
    "        # 划分训练集和测试集\n",
    "        df_for_training = df_sale2[:-12]\n",
    "        df_for_testing = df_sale2[-12:]\n",
    "        df_for_testing1 = df_sale2[-12-12:]\n",
    "\n",
    "        # 归一化\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        df_for_training_scaled = scaler.fit_transform(df_for_training)\n",
    "        df_for_testing_scaled = scaler.transform(df_for_testing)\n",
    "        df_for_testing_scaled1 = scaler.transform(df_for_testing1)\n",
    "\n",
    "        trainX, trainY = createXY(df_for_training_scaled, 12)\n",
    "        testX, testY = createXY(df_for_testing_scaled, 12)\n",
    "        testX1, testY1 = createXY(df_for_testing_scaled1, 12)\n",
    "\n",
    "        # 构建 LSTM 模型\n",
    "        model = Sequential([\n",
    "            LSTM(params['neurons'], activation='relu', return_sequences=True, input_shape=(12, 12)),\n",
    "            LSTM(params['neurons']),\n",
    "            Dropout(params['drop_out']),\n",
    "            Dense(12)\n",
    "        ])\n",
    "\n",
    "        # 编译模型\n",
    "        model.compile(optimizer=Adam(learning_rate=params['learn_rate']),\n",
    "                      loss='mean_squared_error',\n",
    "                      metrics=[MeanAbsolutePercentageError()])\n",
    "\n",
    "        # 训练模型\n",
    "        model.fit(trainX, trainY, epochs=params['epochs'], batch_size=params['batch_size'], verbose=0)\n",
    "\n",
    "        # 测试模型\n",
    "        prediction = model.predict(testX1).reshape(12, 1)\n",
    "        prediction_copies_array = np.repeat(prediction, 12, axis=-1)\n",
    "        pred = scaler.inverse_transform(prediction_copies_array)[:, 0]\n",
    "\n",
    "        df_for_testing['Predictions'] = pred\n",
    "        mape = np.mean(np.abs((df_for_testing['nev_sale'] - df_for_testing['Predictions'])) / df_for_testing['nev_sale']) * 100\n",
    "\n",
    "        # 将当前迭代的结果添加到结果列表中\n",
    "        results.append({\n",
    "            'repetition': repetition + 1,\n",
    "            'neurons': params['neurons'],\n",
    "            'batch_size': params['batch_size'],\n",
    "            'epochs': params['epochs'],\n",
    "            'learn_rate': params['learn_rate'],\n",
    "            'drop_out': params['drop_out'],\n",
    "            'MAPE': mape\n",
    "        })\n",
    "\n",
    "        print(f'Parameters: {params}')\n",
    "        print(f'Test MAPE: {mape}')\n",
    "\n",
    "# 将结果列表转换为 DataFrame 并保存\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_excel('毕业论文数据分析结果//LSTM_MAPE.xlsx', index=False)\n",
    "\n",
    "# 重新定义模型并使用最优参数训练\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "df_scaled = scaler.fit_transform(df_sale2)\n",
    "\n",
    "trainX, trainY = createXY(df_scaled, 12)\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(20, activation='relu', return_sequences=True, input_shape=(12, 12)),\n",
    "    LSTM(20),\n",
    "    Dropout(0.2),\n",
    "    Dense(12)\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# 使用全部训练数据重新训练模型\n",
    "model.fit(trainX, trainY, epochs=200, batch_size=16, verbose=0)\n",
    "\n",
    "# 对所有数据进行预测\n",
    "df_pred = df_scaled[-12:]\n",
    "predX = df_pred.reshape(1, 12, 12)\n",
    "all_prediction = model.predict(predX).reshape(12, 1)\n",
    "all_prediction_copies_array = np.repeat(all_prediction, trainX.shape[2], axis=-1)\n",
    "predicted_sales = scaler.inverse_transform(all_prediction_copies_array)[:, 0]\n",
    "\n",
    "df_nev_sale = df_sale2[['nev_sale']]\n",
    "\n",
    "# 生成 2024 年的日期索引并添加预测数据\n",
    "dates_2024 = pd.date_range(start='2024-01-01', periods=12, freq='M')\n",
    "df_predicted = pd.DataFrame(data=predicted_sales, index=dates_2024, columns=['nev_sale'])\n",
    "df_full = pd.concat([df_nev_sale, df_predicted])\n",
    "\n",
    "# 绘制折线图\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(df_full.index, df_full['nev_sale'], label='Historical and Predicted Sales', color='blue')\n",
    "plt.plot(df_predicted.index, df_predicted['nev_sale'], label='Predicted Sales 2024', color='red', linestyle='--')\n",
    "plt.title('Monthly NEV Sales from 2016 to 2024')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('NEV Sales')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 保存结果并计算 LSTM 变量重要性\n",
    "# 设置 LSTM 特征重要性计算的相关参数\n",
    "COMPUTE_LSTM_IMPORTANCE = 1\n",
    "ONE_FOLD_ONLY = 1\n",
    "NUM_FOLDS = 10\n",
    "\n",
    "# 使用 GPU 进行训练\n",
    "gpu_strategy = tf.distribute.get_strategy()\n",
    "COLS = list(df_for_training.columns)  # 获取训练数据的列名\n",
    "\n",
    "# 在 GPU 环境下进行 K 折交叉验证\n",
    "with gpu_strategy.scope():\n",
    "    kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=2021)\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(trainX, trainY)):\n",
    "        tf.keras.backend.clear_session()  # 清理会话，防止模型重复加载冲突\n",
    "\n",
    "        print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n",
    "\n",
    "        # 划分训练集和验证集\n",
    "        X_train, X_valid = trainX[train_idx], trainX[test_idx]\n",
    "        y_train, y_valid = trainY[train_idx], trainY[test_idx]\n",
    "\n",
    "        # 导入已经训练好的模型\n",
    "        model = keras.models.load_model('C:\\\\Users\\\\lenovo\\\\Desktop\\\\lstm.h5')\n",
    "\n",
    "        # 计算特征重要性\n",
    "        if COMPUTE_LSTM_IMPORTANCE:\n",
    "            results = []\n",
    "            print(' Computing LSTM feature importance...')\n",
    "\n",
    "            # 遍历每个特征列，打乱数据后计算模型性能\n",
    "            for k in tqdm(range(len(COLS))):\n",
    "                if k > 0:\n",
    "                    save_col = X_valid[:, :, k-1].copy()  # 保存原始列\n",
    "                    np.random.shuffle(X_valid[:, :, k-1])  # 随机打乱列\n",
    "\n",
    "                # 预测验证集\n",
    "                oof_preds = model.predict(X_valid, verbose=0).squeeze()\n",
    "                \n",
    "                # 计算 MAPE (Mean Absolute Percentage Error)\n",
    "                mape = np.mean(np.abs((oof_preds - y_valid) / y_valid)) * 100\n",
    "                results.append({'feature': COLS[k], 'mape': mape})\n",
    "\n",
    "                # 恢复被打乱的列\n",
    "                if k > 0:\n",
    "                    X_valid[:, :, k-1] = save_col\n",
    "\n",
    "            # 展示特征重要性\n",
    "            print()\n",
    "            df = pd.DataFrame(results)\n",
    "            df = df.sort_values('mape')\n",
    "            plt.figure(figsize=(8, 8))\n",
    "            plt.barh(np.arange(len(COLS)), df.mape)\n",
    "            plt.yticks(np.arange(len(COLS)), df.feature.values)\n",
    "            plt.title('LSTM Feature Importance', size=16)\n",
    "            plt.ylim((-1, len(COLS)))\n",
    "            plt.show()\n",
    "\n",
    "            # 保存 LSTM 特征重要性到 CSV 文件\n",
    "            df = df.sort_values('mape', ascending=False)\n",
    "            df.to_csv(f'毕业论文数据分析结果/lstm_feature_importance_fold_mape_{fold}.csv', index=False)\n",
    "\n",
    "        # 只进行一次折叠\n",
    "        if ONE_FOLD_ONLY:\n",
    "            break\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
